{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb028e96-3248-4f5e-8dcf-db42eb9cc93b",
   "metadata": {},
   "source": [
    "#### Run all cells to generate the outputs listed below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc38697-010e-4a5e-a1f6-eafffc776400",
   "metadata": {},
   "source": [
    "#### This notebook generates two csv files. \n",
    "1. A csv of all journal articles\n",
    "2. A csv of all articles with duplicate journal titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9c2b0b6f-bcdb-4a11-8da1-97e5e100a4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# get parquet file from here: \n",
    "# Current data source in a S3 bucket: sneakedreferences/processable-references/run_2024_08_19/2024_8_19T15_18_19.parquet\n",
    "# down\n",
    "# The parquet file is downloaded to the local machine\n",
    "FILENAME = \"2024_8_19T15_18_19.parquet\"\n",
    "def read_data(parquet_filename = FILENAME):\n",
    "    \"\"\"Reads parquet file and returns a dataframe\"\"\"\n",
    "    print(f\"{read_data.__name__}: {read_data.__doc__}\")\n",
    "    df = pd.read_parquet(parquet_filename)\n",
    "    return df\n",
    "\n",
    "def rearrange(issn):\n",
    "    \"\"\"Re-arranges ISSN datastructure to a more readable format\"\"\"\n",
    "    data = list(map(lambda x: (f\"{x[0][1]}\", f\"{x[1][1]}\"),issn))\n",
    "    return data\n",
    "\n",
    "def split_issn(issn):\n",
    "    \"\"\"Splits the datastructure into its component issns\"\"\"\n",
    "    e_issn = None\n",
    "    p_issn = None\n",
    "    for i in issn:\n",
    "        if \"print\" in i:\n",
    "            p_issn = i[1]\n",
    "        elif \"electronic\" in i:\n",
    "            e_issn = i[1]\n",
    "    return p_issn, e_issn\n",
    "\n",
    "def fix_issn(row):\n",
    "    \"\"\"Splits the original structure from the filename to a more readable format\"\"\"\n",
    "    issn = row['issn']\n",
    "    first_pass_issn = rearrange(issn)\n",
    "    p_issn, e_issn = split_issn(first_pass_issn)\n",
    "    return p_issn, e_issn\n",
    "\n",
    "def separate_container_title(data):\n",
    "    \"\"\"Converting type to allow for easier grouping\"\"\"\n",
    "    title = data\n",
    "    if isinstance(data, np.ndarray):\n",
    "        title = \", \".join(data.tolist())\n",
    "    return title\n",
    "\n",
    "def prepare_data_frame(filename = FILENAME):\n",
    "    \"\"\"generates dataframe, processes ISSNs, adds counts\"\"\"\n",
    "    print(f\"{prepare_data_frame.__name__}: {prepare_data_frame.__doc__}\")\n",
    "    df = read_data(filename)\n",
    "    df[['print_issn', 'electronic_issn']] = df.apply(fix_issn, axis = 1, result_type='expand')\n",
    "    df['separated_tokens'] = df.token_vocabulary.apply(lambda x: \", \".join(sorted(x)))\n",
    "    df['container_title'] = df.container_title.apply(separate_container_title)\n",
    "    df['ref_pge'] = df.apply(lambda x: x['cleaned_references_length']/x['total_reference_length'], axis=1)\n",
    "    df.drop(columns=['issn'], inplace=True)\n",
    "    # group counts\n",
    "    df['token_counts_by_container_title'] = df.groupby(['separated_tokens', 'container_title'])['DOI'].transform('count')\n",
    "    df['token_counts_by_print_issn'] = df.groupby(['separated_tokens', 'print_issn'])['DOI'].transform('count')\n",
    "    df['token_counts_by_electronic_issn'] = df.groupby(['separated_tokens', 'electronic_issn'])['DOI'].transform('count')\n",
    "    df['container_title_work_type_counts'] = df.groupby(['container_title', 'work_type'])['DOI'].transform('count')\n",
    "    return df\n",
    "\n",
    "def get_journal_articles(df):\n",
    "    \"\"\"generates dataframe that has journal articles where the max token is one of the authors\"\"\"\n",
    "    print(f\"{get_journal_articles.__name__}: {get_journal_articles.__doc__}\")\n",
    "    selected_columns = ['DOI', 'separated_tokens', 'token_frac_refs', 'author', 'flag', 'title', 'container_title',\n",
    "                        'print_issn', 'electronic_issn','ref_pge','total_reference_length','token_counts_by_electronic_issn',\n",
    "                        'token_counts_by_print_issn','token_counts_by_container_title','member']\n",
    "\n",
    "    journal_articles_df = df[(df.work_type == 'journal-article')].sort_values(['token_frac_refs','token_counts_by_electronic_issn', 'token_counts_by_print_issn'], ascending=False)[selected_columns]\n",
    "    return journal_articles_df\n",
    "\n",
    "def output_df(df, filename):\n",
    "   # outputting file\n",
    "    try:\n",
    "        df.to_csv(filename, index=False)\n",
    "    except Exception as e:\n",
    "        print(\"ERROR: \", e)\n",
    "    print(f\"CSV file located here: {filename}\") \n",
    "\n",
    "def prepare_output_df(df, filename):\n",
    "    \"\"\"processes dataframe column headings for better readability, outputs dataframe as a csv file\"\"\"\n",
    "    print(f\"{prepare_output_df.__name__}: {prepare_output_df.__doc__}\")\n",
    "    # renaming columns and removing unnecessary columns\n",
    "    rename_cols = {'separated_tokens': \"most occuring token counted over all processed references\", \n",
    "    'token_frac_refs': \"Percentage of references in which the token(s) appears\", \n",
    "    'flag' :\"author flag\", \n",
    "    'ref_pge': \"Percentage of references that are processed compared to the total number of references in the article\",\n",
    "    'total_reference_length': \"Total no. of references\",\n",
    "    'token_counts_by_electronic_issn': \"token_counts_by_electronic_issn\",\n",
    "    'token_counts_by_print_issn': \"token_counts_by_print_issn\"}\n",
    "    df.drop(['token_counts_by_container_title'], axis=1, inplace=True)\n",
    "    df.rename(columns=rename_cols, inplace=True)\n",
    "    output_df(df, filename)\n",
    "    return df\n",
    "\n",
    "def get_duplicate_titles(journal_articles, filename = \"duplicate_journal_titles.csv\"):\n",
    "    domain_stopwords = ['Preface', 'Introduction', 'Obituary', 'OBITUARY', 'Bibliographie',\n",
    "       'In Memoriam', 'Editorial', 'In memoriam','Dedication',\n",
    "       'Social administration digest', 'Bibliography', 'IN MEMORIAM',\n",
    "       'Obituaries', 'Book review', 'Back Matter',\n",
    "       'References', 'Environmental digest', 'General Assembly',\n",
    "       'Autobiography', 'OBITUARIES', 'Foreword', 'Introduction to the special issue',\n",
    "       'Social Administration Digest', 'News',  'Book Review', 'Tribute', 'Recommended practices', 'Selected bibliography','Selected Bibliography',\n",
    "       'Selected bibliography', 'Présentation','Bericht über Patente','Documentation',\n",
    "       'Results on top physics by CMS', 'Curriculum Vitae', 'NOTES', 'Notes and Comments','Notes','Discussion','Prologue', 'Literatur'\n",
    "       ]\n",
    "    dup_ja = journal_articles[(journal_articles.title.notnull()) & (journal_articles.title != '') & (~ journal_articles.title.isin(domain_stopwords))].copy()\n",
    "    dup_ja = dup_ja[dup_ja.duplicated(subset=['title'], keep=False)].sort_values(['title', 'container_title'])[['DOI', 'title', 'container_title',\"print_issn\", \"electronic_issn\",\"most occuring token counted over all processed references\", \"author flag\", \"member\"]].copy()\n",
    "    output_df(dup_ja, filename)\n",
    "    return dup_ja"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6790c75-fed5-49b8-8780-2a3ba2c1c37f",
   "metadata": {},
   "source": [
    "### Prepare dataframe and get a dataframe of all journal articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "07472ddf-a2f5-4690-a8fe-bb9859e40004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare_data_frame: generates dataframe, processes ISSNs, adds counts\n",
      "read_data: Reads parquet file and returns a dataframe\n",
      "get_journal_articles: generates dataframe that has journal articles where the max token is one of the authors\n",
      "prepare_output_df: processes dataframe column headings for better readability, outputs dataframe as a csv file\n",
      "CSV file located here: journal_articles.csv\n"
     ]
    }
   ],
   "source": [
    "df = prepare_data_frame()\n",
    "journal_articles = get_journal_articles(df)\n",
    "journal_articles = prepare_output_df(journal_articles, \"journal_articles.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0feef31-6241-46f4-965d-66b9a02d2b96",
   "metadata": {},
   "source": [
    "#### Get duplicate journal titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c190cad7-e37d-4e1e-8068-a836695f602b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file located here: duplicate_journal_titles.csv\n"
     ]
    }
   ],
   "source": [
    "dj = get_duplicate_titles(journal_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885bc159-b692-412a-9733-018c37a5aa45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
