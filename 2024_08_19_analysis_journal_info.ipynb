{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c64e0fd0-3db5-4b16-aeec-6c807864075b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# get parquet file from here: \n",
    "# Current data source in a S3 bucket: sneakedreferences/processable-references/run_2024_08_19/2024_8_19T15_18_19.parquet\n",
    "# down\n",
    "def read_data(parquet_filename):\n",
    "    \"\"\"Reads parquet file and returns a dataframe\"\"\"\n",
    "    print(f\"{read_data.__name__}: {read_data.__doc__}\")\n",
    "    df = pd.read_parquet(parquet_filename)\n",
    "    return df\n",
    "\n",
    "def rearrange(issn):\n",
    "    data = list(map(lambda x: (f\"{x[0][1]}\", f\"{x[1][1]}\"),issn))\n",
    "    return data\n",
    "\n",
    "def split_issn(issn):\n",
    "    e_issn = None\n",
    "    p_issn = None\n",
    "    for i in issn:\n",
    "        if \"print\" in i:\n",
    "            p_issn = i[1]\n",
    "        elif \"electronic\" in i:\n",
    "            e_issn = i[1]\n",
    "    return p_issn, e_issn\n",
    "\n",
    "def fix_issn(row):\n",
    "    issn = row['issn']\n",
    "    first_pass_issn = rearrange(issn)\n",
    "    p_issn, e_issn = split_issn(first_pass_issn)\n",
    "    return p_issn, e_issn\n",
    "\n",
    "def separate_container_title(data):\n",
    "    title = data\n",
    "    if isinstance(data, np.ndarray):\n",
    "        title = \", \".join(data.tolist())\n",
    "    return title\n",
    "\n",
    "def prepare_data_frame(df):\n",
    "    \"\"\"processes ISSNs, adds counts\"\"\"\n",
    "    print(f\"{prepare_data_frame.__name__}: {prepare_data_frame.__doc__}\")\n",
    "    df[['print_issn', 'electronic_issn']] = df.apply(fix_issn, axis = 1, result_type='expand')\n",
    "    df['separated_tokens'] = df.token_vocabulary.apply(lambda x: \", \".join(sorted(x)))\n",
    "    df['container_title'] = df.container_title.apply(separate_container_title)\n",
    "    df['ref_pge'] = df.apply(lambda x: x['cleaned_references_length']/x['total_reference_length'], axis=1)\n",
    "    df.drop(columns=['issn'], inplace=True)\n",
    "    # group counts\n",
    "    df['token_counts_by_container_title'] = df.groupby(['separated_tokens', 'container_title'])['DOI'].transform('count')\n",
    "    df['token_counts_by_print_issn'] = df.groupby(['separated_tokens', 'print_issn'])['DOI'].transform('count')\n",
    "    df['token_counts_by_electronic_issn'] = df.groupby(['separated_tokens', 'electronic_issn'])['DOI'].transform('count')\n",
    "    df['container_title_work_type_counts'] = df.groupby(['container_title', 'work_type'])['DOI'].transform('count')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7341226-395e-4511-812d-53656d160934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_data: Reads parquet file and returns a dataframe\n",
      "prepare_data_frame: processes ISSNs, adds counts\n"
     ]
    }
   ],
   "source": [
    "# Reading main dataframe from parquet file\n",
    "df = read_data(\"2024_8_19T15_18_19.parquet\")\n",
    "df = prepare_data_frame(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9c16e5-6b02-48e6-8dc1-8704fe5f4a75",
   "metadata": {},
   "source": [
    "### The following methods get DOI counts per journal from the above dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2f354c2-a2cb-40a7-89a8-4eb0d5bfbab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "api = \"https://api.crossref.org/journals/\"\n",
    "header = {\"mailto\": \"edatta@crossref.org\"}\n",
    "\n",
    "def process_journal_info_df(df):\n",
    "    \"\"\"Gets journal articles from dataframe and adds additional information\"\"\"\n",
    "    print(f\"{process_journal_info_df.__name__}: {process_journal_info_df.__doc__}\")\n",
    "    journal_article_df = df[df.work_type == 'journal-article'][['DOI','container_title', 'print_issn', 'electronic_issn', 'container_title_work_type_counts', 'member']]\n",
    "    journal_article_df['print_issn_counts'] = journal_article_df.groupby('print_issn')['DOI'].transform('count')\n",
    "    journal_article_df['electronic_issn_counts'] = journal_article_df.groupby('electronic_issn')['DOI'].transform('count')\n",
    "    journal_article_df = journal_article_df.sort_values(['electronic_issn_counts','print_issn_counts','container_title_work_type_counts'], ascending=False).drop_duplicates('container_title')\n",
    "    journal_article_df.drop(columns=['DOI'], inplace=True)\n",
    "    e_issn_sort = journal_article_df.sort_values(['electronic_issn_counts', 'print_issn_counts', 'container_title_work_type_counts'], ascending=False).head(60)\n",
    "    p_issn_sort = journal_article_df.sort_values(['print_issn_counts', 'electronic_issn_counts','container_title_work_type_counts'], ascending=False).head(60)\n",
    "    journal_article_df = pd.concat([e_issn_sort,p_issn_sort])\n",
    "    journal_info = journal_article_df.drop_duplicates('container_title').sort_values(['electronic_issn_counts', 'container_title_work_type_counts', 'print_issn'], ascending=False)\n",
    "    return journal_info\n",
    "    \n",
    "def process_issns(journal_info):\n",
    "    \"\"\"Gets all sorts of ISSN info\"\"\"\n",
    "    print(f\"{process_issns.__name__}: {process_issns.__doc__}\")\n",
    "    equal_p_issn_title_counts = list(journal_info[journal_info.container_title_work_type_counts == journal_info.print_issn_counts].index)\n",
    "    \n",
    "    equal_e_issn_title_counts = list(journal_info[journal_info.container_title_work_type_counts == journal_info.electronic_issn_counts].index)\n",
    "    equal_p_issn_actual = list(set(equal_p_issn_title_counts) - set(equal_e_issn_title_counts))\n",
    "    \n",
    "    e_issn_dups = journal_info[(journal_info.duplicated(subset=['electronic_issn'], keep=False)) & (journal_info.electronic_issn.notnull())].index\n",
    "    \n",
    "    p_issn_dups = journal_info[(journal_info.duplicated(subset=['print_issn'], keep=False)) & (journal_info.print_issn.notnull())].index\n",
    "    \n",
    "    p_issn_actual = set(p_issn_dups) - set(e_issn_dups)\n",
    "    p_issn_actual = list(p_issn_actual)\n",
    "    issn_info = {\"print_issn\": p_issn_actual, \"electronic_issn\": list(e_issn_dups), \"equal_e_issn_title_counts\": equal_e_issn_title_counts, \"equal_p_issn_actual\": equal_p_issn_actual}\n",
    "    return issn_info\n",
    "\n",
    "def get_real_counts(row, issn_info):\n",
    "    \"\"\"Processing ISSNs to get true counts per ISSN\"\"\"\n",
    "    print(f\"{get_real_counts.__name__}: {get_real_counts.__doc__}\")\n",
    "    real_count = 0\n",
    "    p_issns = issn_info[\"print_issn\"]\n",
    "    e_issns = issn_info[\"electronic_issn\"]\n",
    "    equal_e_issn_title_counts = issn_info[\"equal_e_issn_title_counts\"]\n",
    "    equal_p_issn_actual = issn_info[\"equal_p_issn_actual\"]\n",
    "    index = row.name\n",
    "    if index in p_issns:\n",
    "        real_count = row['print_issn_counts']\n",
    "    elif index in e_issns:\n",
    "        real_count = row['electronic_issn_counts']\n",
    "    elif index in equal_e_issn_title_counts:\n",
    "        real_count = row['electronic_issn_counts']\n",
    "    elif index in equal_p_issn_actual:\n",
    "        real_count = row['print_issn_counts']\n",
    "    return real_count\n",
    "\n",
    "def remove_dup_issns(journal_info):\n",
    "    \"\"\"Determining which rows to keep for unique journals\"\"\"\n",
    "    print(f\"{remove_dup_issns.__name__}: {remove_dup_issns.__doc__}\")\n",
    "    e = journal_info[journal_info.electronic_issn.notnull()].electronic_issn\n",
    "    e = e.drop_duplicates()\n",
    "    p = journal_info[(journal_info.print_issn.notnull())].print_issn\n",
    "    p_issn_indices = list(set(p.index) - set(e.index))\n",
    "    get_e_issn = journal_info[(journal_info.index.isin(p_issn_indices)) & (journal_info.electronic_issn.notnull())].electronic_issn\n",
    "    peissn = set(get_e_issn.tolist())\n",
    "    eissn = set(e.tolist())\n",
    "    common_issns = list(set(peissn).intersection(eissn))\n",
    "    \n",
    "    for i in common_issns:\n",
    "        index = get_e_issn[get_e_issn == i].index[0]\n",
    "        p_issn_indices.remove(index)\n",
    "    \n",
    "    keep_indices = list(e.index) + p_issn_indices\n",
    "    return keep_indices\n",
    "\n",
    "def get_journal_info(row):\n",
    "    \"\"\"Get journal info from Crossref journals route\"\"\"\n",
    "    print(f\"{get_journal_info.__name__}: {get_journal_info.__doc__}\")\n",
    "    title = None\n",
    "    total_dois = None\n",
    "    issn = None\n",
    "    if row['electronic_issn']:\n",
    "        issn = row['electronic_issn']\n",
    "    elif row['print_issn']:\n",
    "        issn = row['print_issn']\n",
    "    url = api + issn\n",
    "    try:\n",
    "        response = requests.get(url, headers=header)\n",
    "        if response.ok:\n",
    "            rsp = response.json()['message']\n",
    "            title = rsp['title']\n",
    "            total_dois = rsp['counts']['total-dois']\n",
    "        else:\n",
    "            print(f\"ERROR: For issn {issn}: {response.reason}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{issn}: {e}\")\n",
    "    return title, total_dois\n",
    "\n",
    "def null_value(val):\n",
    "    \"\"\"For the final output, adding a string for null values\"\"\"\n",
    "    print(f\"{null_value.__name__}: {null_value.__doc__}\")\n",
    "    new_value = None\n",
    "    if pd.isna(val):\n",
    "        new_value = \"None found\"\n",
    "    else:\n",
    "        new_value = val\n",
    "    return new_value\n",
    "\n",
    "def get_final_output(journal_info):\n",
    "    \"\"\"Re-arranging and re-naming columns\"\"\"\n",
    "    print(f\"{get_final_output.__name__}: {get_final_output.__doc__}\")\n",
    "    columns = ['journal_title', 'total_dois', 'doi_pge']\n",
    "    for c in columns:\n",
    "        journal_info[c] = journal_info[c].apply(null_value)\n",
    "    # re-ordering columns\n",
    "    journal_info = journal_info[['actual_counts', 'journal_title', 'container_title', 'print_issn', 'electronic_issn', 'member', 'total_dois', 'doi_pge']]\n",
    "    rename_fields = {'actual_counts': 'DOI counts', 'journal_title': 'Journal Title in journals route','container_title': 'Container Title from DOI','print_issn': 'Print ISSN', 'electronic_issn': 'Electronic ISSN', 'member': 'Member', 'total_dois': 'Total No. of DOIs', 'doi_pge': 'Percentage of DOI counts over total number of DOIs'}\n",
    "    journal_info.rename(columns=rename_fields, inplace=True)\n",
    "    return journal_info\n",
    "\n",
    "def generate_journal_info(df, filename = \"journal_info_high_counts.csv\"):\n",
    "    \"\"\"Wrapper function to call other functions and outputting dataframe to CSV\"\"\"\n",
    "    print(\"Generating dataframe to show journal information\")\n",
    "    journal_info = process_journal_info_df(df)\n",
    "    issn_info = process_issns(journal_info)\n",
    "    journal_info['actual_counts'] = journal_info.apply(lambda x: get_real_counts(x, issn_info), axis=1)\n",
    "    keep_indices = remove_dup_issns(journal_info)\n",
    "    journal_info.drop(columns=['container_title_work_type_counts', 'print_issn_counts', 'electronic_issn_counts'], inplace=True)\n",
    "    journal_info = journal_info[journal_info.index.isin(keep_indices)].sort_values('actual_counts', ascending=False)\n",
    "    intermediate_journal_info = journal_info.copy()\n",
    "    intermediate_journal_info[['journal_title', 'total_dois']] = intermediate_journal_info.apply(get_journal_info, axis=1, result_type = 'expand')\n",
    "    journal_info = intermediate_journal_info.copy()\n",
    "    journal_info['doi_pge'] = journal_info.apply(lambda r: round((r['actual_counts']/r['total_dois'] * 100),2), axis=1)\n",
    "    journal_info.drop_duplicates(subset=['journal_title'], inplace=True)\n",
    "    journal_info = get_final_output(journal_info)\n",
    "    try:\n",
    "        journal_info.to_csv(filename, index=False)\n",
    "    except Exception as e:\n",
    "        print(\"ERROR: \", e)\n",
    "    print(f\"CSV file located here: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "15a43d2b-b6b2-4c14-8297-696ed72f8aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: For issn 1572-8943: Not Found\n",
      "ERROR: For issn 1572-8943: Not Found\n"
     ]
    }
   ],
   "source": [
    "journal_info = generate_journal_info(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63a1e2b-5f5c-4eac-a3a3-ac20288b22a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputting all journal articles\n",
    "journal_articles_df = df[(df.work_type == 'journal-article')].sort_values(['token_frac_refs','token_counts_by_electronic_issn', 'token_counts_by_print_issn'], ascending=False)[['DOI', 'separated_tokens', 'token_frac_refs', 'author', 'flag', 'title', 'container_title','print_issn', 'electronic_issn','ref_pge','token_counts_by_electronic_issn','token_counts_by_print_issn','token_counts_by_container_title','member']]\n",
    "journal_articles_df.to_csv(\"journal_articles.csv\", index=False)\n",
    "\n",
    "# flagged journal articles\n",
    "flagged_articles = journal_articles_df[(journal_articles_df.flag == \"Yes\") & ((journal_articles_df.token_counts_by_electronic_issn >= 10) | (journal_articles_df.token_counts_by_print_issn >= 10) | (journal_articles_df.token_counts_by_container_title >= 10))].sort_values(['token_counts_by_electronic_issn','token_counts_by_container_title','token_counts_by_print_issn','token_frac_refs'], ascending=False)\n",
    "\n",
    "flagged_articles.to_csv(\"flagged_ja_articles.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
