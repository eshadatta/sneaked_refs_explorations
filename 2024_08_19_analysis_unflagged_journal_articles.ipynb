{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c75bceb-7631-4699-90fd-824fee1a87fb",
   "metadata": {},
   "source": [
    "### Run all cells and it will generate a csv file\n",
    "### This notebook generates a dataframe of journal articles where the most common token is not one of the authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "9c2b0b6f-bcdb-4a11-8da1-97e5e100a4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# get parquet file from here: \n",
    "# Current data source in a S3 bucket: sneakedreferences/processable-references/run_2024_08_19/2024_8_19T15_18_19.parquet\n",
    "# down\n",
    "FILENAME = \"2024_8_19T15_18_19.parquet\"\n",
    "def read_data(parquet_filename = FILENAME):\n",
    "    \"\"\"Reads parquet file and returns a dataframe\"\"\"\n",
    "    print(f\"{read_data.__name__}: {read_data.__doc__}\")\n",
    "    df = pd.read_parquet(parquet_filename)\n",
    "    return df\n",
    "\n",
    "def rearrange(issn):\n",
    "    \"\"\"Re-arranges ISSN datastructure to a more readable format\"\"\"\n",
    "    data = list(map(lambda x: (f\"{x[0][1]}\", f\"{x[1][1]}\"),issn))\n",
    "    return data\n",
    "\n",
    "def split_issn(issn):\n",
    "    \"\"\"Splits the datastructure into its component issns\"\"\"\n",
    "    e_issn = None\n",
    "    p_issn = None\n",
    "    for i in issn:\n",
    "        if \"print\" in i:\n",
    "            p_issn = i[1]\n",
    "        elif \"electronic\" in i:\n",
    "            e_issn = i[1]\n",
    "    return p_issn, e_issn\n",
    "\n",
    "def fix_issn(row):\n",
    "    \"\"\"Splits the original structure from the filename to a more readable format\"\"\"\n",
    "    issn = row['issn']\n",
    "    first_pass_issn = rearrange(issn)\n",
    "    p_issn, e_issn = split_issn(first_pass_issn)\n",
    "    return p_issn, e_issn\n",
    "\n",
    "def separate_container_title(data):\n",
    "    \"\"\"Converting type to allow for easier grouping\"\"\"\n",
    "    title = data\n",
    "    if isinstance(data, np.ndarray):\n",
    "        title = \", \".join(data.tolist())\n",
    "    return title\n",
    "\n",
    "def prepare_data_frame(filename = FILENAME):\n",
    "    \"\"\"generates dataframe, processes ISSNs, adds counts\"\"\"\n",
    "    print(f\"{prepare_data_frame.__name__}: {prepare_data_frame.__doc__}\")\n",
    "    df = read_data(filename)\n",
    "    df[['print_issn', 'electronic_issn']] = df.apply(fix_issn, axis = 1, result_type='expand')\n",
    "    df['separated_tokens'] = df.token_vocabulary.apply(lambda x: \", \".join(sorted(x)))\n",
    "    df['container_title'] = df.container_title.apply(separate_container_title)\n",
    "    df['ref_pge'] = df.apply(lambda x: x['cleaned_references_length']/x['total_reference_length'], axis=1)\n",
    "    df.drop(columns=['issn'], inplace=True)\n",
    "    # group counts\n",
    "    df['token_counts_by_container_title'] = df.groupby(['separated_tokens', 'container_title'])['DOI'].transform('count')\n",
    "    df['token_counts_by_print_issn'] = df.groupby(['separated_tokens', 'print_issn'])['DOI'].transform('count')\n",
    "    df['token_counts_by_electronic_issn'] = df.groupby(['separated_tokens', 'electronic_issn'])['DOI'].transform('count')\n",
    "    df['container_title_work_type_counts'] = df.groupby(['container_title', 'work_type'])['DOI'].transform('count')\n",
    "    return df\n",
    "\n",
    "def get_unflagged_journal_articles(df):\n",
    "    \"\"\"generates dataframe that has journal articles where the max token is not one of the authors\"\"\"\n",
    "    print(f\"{get_unflagged_journal_articles.__name__}: {get_unflagged_journal_articles.__doc__}\")\n",
    "    selected_columns = ['DOI', 'separated_tokens', 'token_frac_refs', 'author', 'flag', 'title', 'container_title',\n",
    "                        'print_issn', 'electronic_issn','ref_pge','total_reference_length','token_counts_by_electronic_issn',\n",
    "                        'token_counts_by_print_issn','token_counts_by_container_title','member']\n",
    "    # getting unflagged journal articles with token counts that are greater than 10. \n",
    "    # Token counts are the number of repeated tokens grouped by various types of ISSNs. \n",
    "    # This shows the number of possible authors that are repeated by journal\n",
    "    journal_articles_df = df[(df.work_type == 'journal-article')].sort_values(['token_frac_refs','token_counts_by_electronic_issn', 'token_counts_by_print_issn'], ascending=False)[selected_columns]\n",
    "    unflagged_journal_articles = journal_articles_df[(journal_articles_df.flag == \"No\") & \n",
    "                                                     ((journal_articles_df.token_counts_by_electronic_issn >= 10) | \n",
    "                                                      (journal_articles_df.token_counts_by_print_issn >= 10) | \n",
    "                                                      (journal_articles_df.token_counts_by_container_title >= 10))].sort_values(['token_counts_by_electronic_issn',\n",
    "                                                                                                                                 'token_counts_by_container_title',\n",
    "                                                                                                                                 'token_counts_by_print_issn','token_frac_refs'], ascending=False)\n",
    "    return unflagged_journal_articles\n",
    "\n",
    "def get_most_common_tokens(unflagged_journal_articles):\n",
    "    \"\"\"gets the most common tokens\"\"\"\n",
    "    print(f\"{get_most_common_tokens.__name__}: {get_most_common_tokens.__doc__}\")\n",
    "    tokens = []\n",
    "    columns = [\"token_counts_by_electronic_issn\", \"token_counts_by_print_issn\", \"token_counts_by_container_title\"]\n",
    "    for c in columns:\n",
    "        st = list(set(list(unflagged_journal_articles[unflagged_journal_articles[c] == unflagged_journal_articles[c].max()].separated_tokens)))\n",
    "        tokens.extend(st)\n",
    "    return list(set(tokens))\n",
    "\n",
    "def prepare_output_df(unflagged_journal_articles, filename = 'unflagged_journal_articles.csv'):\n",
    "    \"\"\"processes dataframe column headings for better readability, outputs dataframe as a csv file\"\"\"\n",
    "    print(f\"{prepare_output_df.__name__}: {prepare_output_df.__doc__}\")\n",
    "    # renaming columns and removing unnecessary columns\n",
    "    rename_cols = {'separated_tokens': \"most occuring token counted over all processed references\", \n",
    "    'token_frac_refs': \"Percentage of references in which the token(s) appears\", \n",
    "    'flag' :\"author flag\", \n",
    "    'ref_pge': \"Percentage of references that are processed compared to the total number of references in the article\",\n",
    "    'total_reference_length': \"Total no. of references\",\n",
    "    'token_counts_by_electronic_issn': \"token_counts_by_electronic_issn\",\n",
    "    'token_counts_by_print_issn': \"token_counts_by_print_issn\"}\n",
    "    unflagged_journal_articles.drop(['token_counts_by_container_title'], axis=1, inplace=True)\n",
    "    unflagged_journal_articles.rename(columns=rename_cols, inplace=True)\n",
    "    # outputting file\n",
    "    try:\n",
    "        unflagged_journal_articles.to_csv(filename, index=False)\n",
    "    except Exception as e:\n",
    "        print(\"ERROR: \", e)\n",
    "    print(f\"CSV file located here: {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6790c75-fed5-49b8-8780-2a3ba2c1c37f",
   "metadata": {},
   "source": [
    "### Prepare dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "e8eceb29-3a2d-479f-8d88-d7e2cf40479f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare_data_frame: generates dataframe, processes ISSNs, adds counts\n",
      "read_data: Reads parquet file and returns a dataframe\n",
      "get_unflagged_journal_articles: generates dataframe that has journal articles where the max token is not one of the authors\n",
      "get_most_common_tokens: gets the most common tokens\n"
     ]
    }
   ],
   "source": [
    "df = prepare_data_frame()\n",
    "unflagged_journal_articles = get_unflagged_journal_articles(df)\n",
    "\n",
    "most_common_tokens = get_most_common_tokens(unflagged_journal_articles)\n",
    "# currently, the output of most_common_tokens is\n",
    "# ['Simos', 'Medicine, Science, Sports']\n",
    "\n",
    "# removing tokens that are 'Medicine, Science, Sports' as this is not a person\n",
    "all_separated_tokens = list(set(unflagged_journal_articles[unflagged_journal_articles.separated_tokens != 'Medicine, Science, Sports'].sort_values('token_frac_refs', ascending=False).separated_tokens.to_list()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16936039-5f9e-4993-898f-c3c16a03bf9f",
   "metadata": {},
   "source": [
    "### Process separated tokens to determine which tokens might be a person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "0321a2a9-5241-4388-a665-0db9d2b6bb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# got this list after querying chatgpt\n",
    "# queried chatgpt open ai model 4o mini just using the chat prompt\n",
    "# Identify the people,  and output the answer as a python list from this list: all_separated_tokens\n",
    "# this needs to get better, currently it's a very manual process\n",
    "# Identify the people from this list from the variables all_separated_tokens,  and output the answer as a python list.  \n",
    "# I also ran a few experiments using Stanza's NER models and narrowed the tokens to this\n",
    "# This is an extremely manual process and further experiments need to be done to make it less manual\n",
    "# One way would be fine tuning a model with author names in Crossref to narrow down the possibilities of who might be a \"person\"\n",
    "#Response\n",
    "people = ['محمد',\n",
    "'ANZ, Rashed',\n",
    " 'Neur',\n",
    " 'CW, Shu',\n",
    " 'Yamaguchi',\n",
    " 'Merz',\n",
    " 'Wehner',\n",
    " 'Yousof',\n",
    " 'Rashed',\n",
    " 'Perrotta',\n",
    " 'Jacques',\n",
    " 'Grzegorz, Michalski',\n",
    " 'Aithal',\n",
    " 'Terziev',\n",
    " 'Sheikholeslami',\n",
    " 'Huang',\n",
    " 'Hua',\n",
    " 'Hashim',\n",
    " 'Bondur',\n",
    " 'Dziewonski',\n",
    " 'Mamatov',\n",
    " 'Tsvetkov',\n",
    " 'Abu',\n",
    " 'Ulenikov',\n",
    " 'Latash',\n",
    " 'Stević',\n",
    " 'Carraher',\n",
    " 'Tezduyar',\n",
    " 'Inoue',\n",
    " 'Gayda',\n",
    " 'BC, Yang',\n",
    " 'Degadwala',\n",
    " 'Hipel',\n",
    " 'Ghassan',\n",
    " 'Mahmoud',\n",
    " 'Fischer',\n",
    " 'Dietz',\n",
    " 'Samhita',\n",
    " 'Wyrok',\n",
    " 'Sathish',\n",
    " 'Okubo',\n",
    " 'Karthikeyan',\n",
    " 'Guz',\n",
    " 'Kim',\n",
    " 'Wang',\n",
    " 'Simos',\n",
    " 'Medvedev',\n",
    " 'Mohanavel',\n",
    " 'Pivinskii',\n",
    " 'Bobtelsky',\n",
    " 'Arian',\n",
    " 'Ruggeri',\n",
    " 'Bohlmann',\n",
    " 'Makarov',\n",
    " 'Ding',\n",
    " 'Roukos',\n",
    " 'Jefferson']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db92932a-60f5-4cbf-a153-e7c36ff1ea05",
   "metadata": {},
   "source": [
    "### Get unflagged journal articles rows that only contain tokens that have been identified as people, prepare dataframe for output, and output as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "e418e96c-3cf3-4165-96fa-a4216780c94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare_output_df: processes dataframe column headings for better readability, outputs dataframe as a csv file\n",
      "CSV file located here: unflagged_journal_articles.csv\n"
     ]
    }
   ],
   "source": [
    "# getting only those rows where the tokens have been identified as people\n",
    "unflagged_journal_articles = unflagged_journal_articles[unflagged_journal_articles.separated_tokens.isin(people)]\n",
    "prepare_output_df(unflagged_journal_articles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
